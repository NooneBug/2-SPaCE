#define here all constants
# DEFAULT will provide base values for constants

#dataset
[DEFAULT]
LEFT_CONTEXT = left_context_tokens
MENTION = mention
RIGHT_CONTEXT = right_context_tokens
LABELS = labels
TRAIN_DATASET_PATH = toy_datasets/toy_simple_input
VAL_DATASET_PATH = toy_datasets/toy_simple_input_val
TEST_DATATSET_PATH = toy_datasets/toy_simple_input_test
TYPE_SPACE_NUMBER = 2
WORD_EMBEDDING_CONFIG = GLOVE
TYPE_EMBEDDING_CONFIG = MultiTypeEmbedding
TYPE_PADDING_INDEX = -1

[GLOVE]
WORD_EMBEDDING_PATH = resources/word_embeddings/toy_glove.txt

[MultiTypeEmbedding]
# explicit the embedding configurations through EMBEDDING_CONFIGS
# use config names separated by one single space
EMBEDDING_CONFIGS = NICKEL TYPE2VEC
PADDING_INDEX = -1

[NICKEL]
EMBEDDING_NAME = nickel_hyperbolic
PATH = resources/type_embeddings/toy_nickel.pth
EMBEDDING_CLASS_NAME = NickelEmbedding

[TYPE2VEC]
EMBEDDING_NAME = type2vec
PATH = resources/type_embeddings/toy_t2v
EMBEDDING_CLASS_NAME = Type2VecEmbedding

[2-SPACE MODULES CONFIGS]
; CLASS = ComposedRegressiveNetwork
CLASS = ComposedClassificationNetwork
WORD_MANIPULATION_MODULE = SHIMAOKA
DEEP_NETWORK = COMMON
PROJECTORS = MultiProjectorsManager
CLASSIFIER = CLASSIFIER
LOSS = MultiLossManager
EVALUATOR = MultiEvaluatorManager

[SHIMAOKA]
CLASS = ShimaokaModel
DATALOADER_CLASS = ShimaokaDataLoader
# OUTPUT_SIZE = context_rnn_size * 2 + emb_size + char_emb_size 
OUTPUT_SIZE = 750
PADDING_INDEX = -1
CHAR_PAD = <pad>
context_rnn_size = 200
emb_size = 300
char_emb_size = 50
mention_dropout_size = 0.5
positional_emb_size = 25
context_dropout = 0.2
mention_length = 5
max_char_in_mention = 25

[COMMON]
CLASS = CommonNetwork
INPUT_SIZE = 750
layers = 512 512
dropout_prob = 0.2

[MultiProjectorsManager]
CLASS = MultiProjectorsManager
PROJECTOR_CONFIGS = HYPERBOLIC_PROJECTOR COSINE_PROJECTOR

[HYPERBOLIC_PROJECTOR]
CLASS = HyperbolicProjector
; this name have to match with on of the 'EMBEDDING_NAME' values in the TYPE_EMBEDDING_CONFIG configs 
NAME = nickel_hyperbolic 
layers = 256 256 10
input_size = 512

[COSINE_PROJECTOR]
CLASS = CosineProjector
; this name have to match with on of the 'EMBEDDING_NAME' values in the TYPE_EMBEDDING_CONFIG configs 
NAME = type2vec
layers = 256 128 10
input_size = 512

[CLASSIFIER]
CLASS = Classifier
LOSS = BCELoss
layers = 256
input_size = 532

[MultiLossManager]
CLASS = MultiLossManager
LOSSES = Hyperbolic_Loss Cosine_Loss
weights = 0.5 0.5

[Hyperbolic_Loss]
CLASS = multilabel_Minimum_Poincare
; this name have to match with on of the 'EMBEDDING_NAME' values in the TYPE_EMBEDDING_CONFIG configs 
NAME = nickel_hyperbolic

[Cosine_Loss]
CLASS = multilabel_Minimum_Cosine
; this name have to match with on of the 'EMBEDDING_NAME' values in the TYPE_EMBEDDING_CONFIG configs 
NAME = type2vec

[TRAINING_PARAMETERS]
train_batch_size = 1024
val_batch_size = 1024
test_batch_size = 1024
shuffle = False
optimizer = RIEMANNIAN_ADAM
epochs = 10

[RIEMANNIAN_ADAM]
class = riemannianAdamOptimizer
learning_rate = 1e-3

[MultiEvaluatorManager]
CLASS = MultiEvaluatorManager